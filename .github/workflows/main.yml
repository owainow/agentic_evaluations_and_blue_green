name: "AI Agent Evaluation"

on:
  workflow_dispatch:
    inputs:
      project_endpoint:
        description: 'Azure AI Foundry project endpoint'
        required: true
        type: string
      deployment_name:
        description: 'Model deployment name'
        required: true
        type: string
      agent_id:
        description: 'Agent ID to evaluate'
        required: true
        type: string
      data_file:
        description: 'Evaluation data file'
        required: false
        type: string
        default: 'weather_news_test.json'
      function_app_url:
        description: 'Azure Function App URL'
        required: false
        type: string
  workflow_call:
    inputs:
      project_endpoint:
        description: 'Azure AI Foundry project endpoint'
        required: true
        type: string
      deployment_name:
        description: 'Model deployment name'
        required: true
        type: string
      agent_id:
        description: 'Agent ID to evaluate'
        required: true
        type: string
      data_file:
        description: 'Evaluation data file'
        required: false
        type: string
        default: 'weather_news_test.json'
      function_app_url:
        description: 'Azure Function App URL'
        required: false
        type: string
    secrets:
      AZURE_CLIENT_ID:
        required: true
      AZURE_TENANT_ID:
        required: true
      AZURE_SUBSCRIPTION_ID:
        required: true

permissions:
  id-token: write
  contents: read

jobs:
  run-action:
    name: AI Foundry Agent Evaluation
    runs-on: ubuntu-latest
    outputs:
      evaluation-passed: ${{ steps.check-results.outputs.passed }}
      evaluation-summary: ${{ steps.create-summary.outputs.summary }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Azure login using Federated Credentials
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies for function implementations
        run: |
          pip install requests

      - name: Run AI Agent Evaluation
        id: ai-agent-evaluation
        continue-on-error: true
        uses: microsoft/ai-agent-evals@v2-beta
        with:
          azure-ai-project-endpoint: ${{ inputs.project_endpoint }}
          deployment-name: ${{ inputs.deployment_name }}
          agent-ids: ${{ inputs.agent_id }}
          data-path: ${{ github.workspace }}/evaluations/${{ inputs.data_file }}

      - name: Check evaluation results
        id: check-results
        run: |
          if [ "${{ steps.ai-agent-evaluation.outcome }}" == "success" ]; then
            echo "passed=true" >> $GITHUB_OUTPUT
            echo "âœ… AI Agent evaluation completed successfully"
          else
            echo "passed=false" >> $GITHUB_OUTPUT
            echo "âŒ AI Agent evaluation failed"
          fi

      - name: Create evaluation summary
        id: create-summary
        run: |
          SUMMARY="AI Agent Evaluation completed on $(date). "
          SUMMARY+="Agent: ${{ inputs.agent_id }}. "
          SUMMARY+="Model: ${{ inputs.deployment_name }}. "
          SUMMARY+="Status: ${{ steps.ai-agent-evaluation.outcome }}. "
          SUMMARY+="Test data: ${{ inputs.data_file }}. "
          SUMMARY+="Evaluators: Tool calling, intent resolution, task adherence, RAG, and safety."
          
          echo "summary=${SUMMARY}" >> $GITHUB_OUTPUT
          
          echo "## ðŸ¤– AI Agent Evaluation Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Agent ID**: ${{ inputs.agent_id }}" >> $GITHUB_STEP_SUMMARY
          echo "**Model Deployment**: ${{ inputs.deployment_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Test Data**: ${{ inputs.data_file }}" >> $GITHUB_STEP_SUMMARY
          echo "**Status**: ${{ steps.ai-agent-evaluation.outcome == 'success' && 'âœ… Success' || 'âŒ Failed' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Agent-Specific Evaluators:" >> $GITHUB_STEP_SUMMARY
          echo "- **Tool Calling**: ToolCallAccuracyEvaluator" >> $GITHUB_STEP_SUMMARY
          echo "- **Intent**: IntentResolutionEvaluator" >> $GITHUB_STEP_SUMMARY
          echo "- **Task Adherence**: TaskAdherenceEvaluator" >> $GITHUB_STEP_SUMMARY
          echo "- **RAG**: RelevanceEvaluator, GroundednessEvaluator" >> $GITHUB_STEP_SUMMARY
          echo "- **Safety**: ViolenceEvaluator" >> $GITHUB_STEP_SUMMARY
          echo "- **Quality**: CoherenceEvaluator" >> $GITHUB_STEP_SUMMARY

  json-validation:
    name: JSON Response Validation
    runs-on: ubuntu-latest
    needs: run-action  # Run after AI Foundry Evaluation completes to avoid rate limiting
    steps:
      - name: Wait for rate limit cooldown
        run: sleep 30  # Brief pause to avoid rate limiting issues
        
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Azure login using Federated Credentials
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Install Azure AI SDK
        run: |
          pip install azure-ai-projects==1.0.0 azure-ai-agents==1.1.0 azure-identity

      - name: Run JSON Validation Tests
        continue-on-error: true
        env:
          PROJECT_ENDPOINT: ${{ inputs.project_endpoint }}
          AGENT_ID: ${{ inputs.agent_id }}
          AGENT_NAME: 'Weather News Agent'
          FUNCTION_APP_URL: ${{ inputs.function_app_url }}
        run: |
          python scripts/validate_json_responses_azure_functions.py

      - name: Upload JSON Validation Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: json-validation-results
          path: json_validation_results_azure_functions.json
          retention-days: 30
