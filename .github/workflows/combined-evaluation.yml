name: "Combined AI Evaluation (Agent + GenAI + Functional)"

on:
  workflow_call:
    inputs:
      # Agent evaluation inputs
      project_endpoint:
        description: 'Azure AI Foundry project endpoint'
        required: true
        type: string
      
      deployment_name:
        description: 'Primary model deployment name'
        required: true
        type: string
      
      deployment_name_2:
        description: 'Second model deployment name (for comparison)'
        required: false
        type: string
      
      agent_id:
        description: 'Primary agent ID to evaluate'
        required: true
        type: string
      
      agent_id_2:
        description: 'Second agent ID to evaluate (for comparison)'
        required: false
        type: string
      
      function_app_url:
        description: 'Azure Function App URL for tools'
        required: true
        type: string
      
      # GenAI evaluation inputs
      azure_endpoint:
        description: 'Azure OpenAI endpoint'
        required: true
        type: string
      
      api_version:
        description: 'Azure OpenAI API version'
        required: true
        type: string
        default: '2024-10-21'
      
      # Test data files
      agent_data_file:
        description: 'Agent evaluation test data file'
        required: true
        type: string
        default: 'weather_test.json'
      
      genai_data_file:
        description: 'GenAI evaluation test data file'
        required: true
        type: string
        default: 'genai_evaluation_test.jsonl'

    secrets:
      AZURE_CLIENT_ID:
        required: true
      AZURE_TENANT_ID:
        required: true
      AZURE_SUBSCRIPTION_ID:
        required: true
      AZURE_OPENAI_API_KEY:
        required: true

  workflow_dispatch:
    inputs:
      # Agent evaluation inputs
      project_endpoint:
        description: 'Azure AI Foundry project endpoint'
        required: true
        type: string
      
      deployment_name:
        description: 'Model deployment name'
        required: true
        type: string
      
      agent_id:
        description: 'Agent ID to evaluate'
        required: true
        type: string
      
      function_app_url:
        description: 'Azure Function App URL for tools'
        required: true
        type: string
      
      # GenAI evaluation inputs
      azure_endpoint:
        description: 'Azure OpenAI endpoint'
        required: true
        type: string
      
      api_version:
        description: 'Azure OpenAI API version'
        required: true
        type: string
        default: '2024-10-21'
      
      # Test data files
      agent_data_file:
        description: 'Agent evaluation test data file'
        required: true
        type: string
        default: 'weather_test.json'
      
      genai_data_file:
        description: 'GenAI evaluation test data file'
        required: true
        type: string
        default: 'genai_evaluation_test.json'

permissions:
  id-token: write
  contents: read

jobs:
  functional-json-testing:
    name: "ðŸ§ª Functional JSON Validation Testing"
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install azure-ai-projects azure-identity azure-ai-agents requests
      
      - name: Run JSON Validation Tests
        env:
          PROJECT_ENDPOINT: ${{ inputs.project_endpoint }}
          AGENT_ID: ${{ inputs.agent_id }}
          FUNCTION_APP_URL: ${{ inputs.function_app_url }}
        run: |
          echo "ðŸ§ª Running JSON validation tests..."
          echo "Testing agent JSON responses and Azure Functions integration..."
          python scripts/validate_json_responses_azure_functions.py
      
      - name: Upload JSON Test Results
        uses: actions/upload-artifact@v4
        with:
          name: json-validation-results
          path: |
            json_validation_results_*.json
            **/*validation*.json
          retention-days: 30
        if: always()

  agent-evaluation:
    name: "ðŸ¤– AI Agent Evaluation"
    runs-on: ubuntu-latest
    needs: functional-json-testing
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      
      - name: Run Agent Evaluation
        uses: microsoft/ai-agent-evals@v2-beta
        with:
          azure-ai-project-endpoint: ${{ inputs.project_endpoint }}
          deployment-name: ${{ inputs.deployment_name }}
          agent-ids: ${{ inputs.agent_id_2 != '' && format('{0},{1}', inputs.agent_id, inputs.agent_id_2) || inputs.agent_id }}
          data-path: ${{ github.workspace }}/evaluations/${{ inputs.agent_data_file }}

  genai-evaluation:
    name: "ðŸ§  GenAI Model Evaluation"
    runs-on: ubuntu-latest
    needs: functional-json-testing
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      
      - name: Create Primary GenAI Evaluation Config
        env:
          AZURE_OPENAI_ENDPOINT: ${{ inputs.azure_endpoint }}
          AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
          AZURE_OPENAI_DEPLOYMENT: ${{ inputs.deployment_name }}
          AZURE_OPENAI_API_VERSION: ${{ inputs.api_version }}
        run: |
          cat > genai-eval-config-primary.json <<EOF
          {
            "data": "${{ github.workspace }}/evaluations/${{ inputs.genai_data_file }}",
            "evaluators": {
              "coherence": "CoherenceEvaluator",
              "fluency": "FluencyEvaluator",
              "groundedness": "GroundednessEvaluator",
              "relevance": "RelevanceEvaluator"
            },
            "ai_model_configuration": {
              "type": "azure_openai",
              "azure_endpoint": "${AZURE_OPENAI_ENDPOINT}",
              "azure_deployment": "${AZURE_OPENAI_DEPLOYMENT}",
              "api_key": "${AZURE_OPENAI_API_KEY}",
              "api_version": "${AZURE_OPENAI_API_VERSION}"
            }
          }
          EOF
      
      - name: Run Primary GenAI Model Evaluation
        uses: microsoft/genai-evals@main
        with:
          evaluate-configuration: ${{ github.workspace }}/genai-eval-config-primary.json
          
      - name: Create Comparison GenAI Evaluation Config
        if: ${{ inputs.deployment_name_2 != '' }}
        env:
          AZURE_OPENAI_ENDPOINT: ${{ inputs.azure_endpoint }}
          AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
          AZURE_OPENAI_DEPLOYMENT: ${{ inputs.deployment_name_2 }}
          AZURE_OPENAI_API_VERSION: ${{ inputs.api_version }}
        run: |
          cat > genai-eval-config-comparison.json <<EOF
          {
            "data": "${{ github.workspace }}/evaluations/${{ inputs.genai_data_file }}",
            "evaluators": {
              "coherence": "CoherenceEvaluator",
              "fluency": "FluencyEvaluator",
              "groundedness": "GroundednessEvaluator",
              "relevance": "RelevanceEvaluator"
            },
            "ai_model_configuration": {
              "type": "azure_openai",
              "azure_endpoint": "${AZURE_OPENAI_ENDPOINT}",
              "azure_deployment": "${AZURE_OPENAI_DEPLOYMENT}",
              "api_key": "${AZURE_OPENAI_API_KEY}",
              "api_version": "${AZURE_OPENAI_API_VERSION}"
            }
          }
          EOF
          
      - name: Run Comparison GenAI Model Evaluation
        if: ${{ inputs.deployment_name_2 != '' }}
        uses: microsoft/genai-evals@main
        with:
          evaluate-configuration: ${{ github.workspace }}/genai-eval-config-comparison.json

  combined-summary:
    name: "ðŸ“Š Combined Results Summary"
    runs-on: ubuntu-latest
    needs: [functional-json-testing, agent-evaluation, genai-evaluation]
    if: always()
    
    steps:
      - name: Evaluation Summary
        run: |
          if [ "${{ inputs.agent_id_2 }}" != "" ]; then
            echo "## ðŸŽ¯ Multi-Agent Comparison Evaluation Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Primary Agent:** ${{ inputs.agent_id }} (Model: ${{ inputs.deployment_name }})" >> $GITHUB_STEP_SUMMARY
            echo "**Comparison Agent:** ${{ inputs.agent_id_2 }} (Model: ${{ inputs.deployment_name_2 }})" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          else
            echo "## ðŸŽ¯ Combined Evaluation Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "| Component | Status | Result |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ§ª JSON Validation | ${{ needs.functional-json-testing.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} | Functional testing of agent JSON responses |" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ inputs.agent_id_2 }}" != "" ]; then
            echo "| ðŸ¤– Agent Comparison | ${{ needs.agent-evaluation.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} | Comparative evaluation between agents |" >> $GITHUB_STEP_SUMMARY
            echo "| ðŸ§  Model Comparison | ${{ needs.genai-evaluation.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} | Comparative model quality evaluation |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| ðŸ¤– AI Agent | ${{ needs.agent-evaluation.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} | Agent evaluation with tool calling tests |" >> $GITHUB_STEP_SUMMARY
            echo "| ðŸ§  GenAI Model | ${{ needs.genai-evaluation.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} | Model quality evaluation with ground truth |" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Overall status
          if [ "${{ needs.functional-json-testing.result }}" == "success" ] && [ "${{ needs.agent-evaluation.result }}" == "success" ] && [ "${{ needs.genai-evaluation.result }}" == "success" ]; then
            if [ "${{ inputs.agent_id_2 }}" != "" ]; then
              echo "### ðŸŽ‰ Overall Result: âœ… **ALL COMPARISON EVALUATIONS PASSED**" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "All functional, agent comparison, and model comparison evaluations completed successfully. Review detailed results to compare model performance!" >> $GITHUB_STEP_SUMMARY
            else
              echo "### ðŸŽ‰ Overall Result: âœ… **ALL EVALUATIONS PASSED**" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "All functional, agent, and model evaluations completed successfully. Ready for deployment!" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "### âš ï¸ Overall Result: âŒ **SOME EVALUATIONS FAILED**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "One or more evaluations failed. Please review the results before proceeding." >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“ Detailed Results" >> $GITHUB_STEP_SUMMARY
          echo "- **JSON Validation**: Check 'json-validation-results' artifact for functional test results" >> $GITHUB_STEP_SUMMARY
          echo "- **Agent Evaluation**: Check the individual evaluation summaries for tool calling metrics" >> $GITHUB_STEP_SUMMARY
          echo "- **GenAI Evaluation**: Check the individual evaluation summaries for model quality metrics" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸš€ Next Steps" >> $GITHUB_STEP_SUMMARY
          if [ "${{ needs.functional-json-testing.result }}" == "success" ] && [ "${{ needs.agent-evaluation.result }}" == "success" ] && [ "${{ needs.genai-evaluation.result }}" == "success" ]; then
            echo "- âœ… Ready for production deployment" >> $GITHUB_STEP_SUMMARY
            echo "- âœ… JSON responses validated" >> $GITHUB_STEP_SUMMARY
            echo "- âœ… Agent functionality confirmed" >> $GITHUB_STEP_SUMMARY
            echo "- âœ… Model quality verified" >> $GITHUB_STEP_SUMMARY
          else
            echo "- âš ï¸ Review failed evaluations before deployment" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ“‹ Check individual evaluation logs and artifacts" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ”§ Consider agent, function, or model adjustments" >> $GITHUB_STEP_SUMMARY
          fi